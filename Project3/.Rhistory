# Create hist of horsepwr with binwidth of 60
cars %>%
ggplot(aes(horsepwr)) +
geom_histogram(binwidth = 60) +
ggtitle("First histogram")
# Create hist of horsepwr with binwidth of 3
cars %>%
ggplot(aes(horsepwr)) +
geom_histogram(binwidth = 3) +
ggtitle("binwidth=3 histogram")
# Create hist of horsepwr with binwidth of 30
cars %>%
ggplot(aes(horsepwr)) +
geom_histogram(binwidth = 30) +
ggtitle("binwidth=30 histogram")
# Create hist of horsepwr with binwidth of 60
cars %>%
ggplot(aes(horsepwr)) +
geom_histogram(binwidth = 60) +
ggtitle("binwidth=60 histogram")
# Construct box plot of msrp
cars %>%
ggplot(aes(x = 1, y = msrp)) +
geom_boxplot()
# Exclude outliers from data
cars_no_out <- cars %>%
filter(msrp < 100000)
# Construct box plot of msrp using the reduced dataset
cars_no_out %>%
ggplot(aes(x = 1, y = msrp)) +
geom_boxplot()
# Create plot of city_mpg
cars %>%
ggplot(aes(x = 1, y = city_mpg)) +
geom_boxplot()
# Create plot of width
cars %>%
ggplot(aes(x = 1, y = width)) +
geom_boxplot()
knitr::opts_chunk$set(echo = TRUE)
set.seed(2489)
ourdata = rnorm(1000. mean=600, sd=80)
set.seed(2489)
ourdata <- rnorm(1000, mean=600, sd=80)
hist(ourdata)
set.seed(2489)
ourdata <- rnorm(1000, mean=600, sd=80)
#hist(ourdata)
sortedobject <- ifelse(ourdata > 700, "yes", "no")
table(sortedobject)
totalnumber=1000
yes = 102
prop.test(x=yes, n=totalnumber,alternative="two.sided")
knitr::opts_chunk$set(echo = TRUE)
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# in the code below, n cannot be > 27 because english alphabet ;)
# set simulation vars
set.seed(643)
n <- 15
percentNAs <- .3
probs <- c(1-percentNAs, percentNAs)
users <- paste("User_", LETTERS[1:n], sep="")
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
toyDF <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#toyDF[,1:3] <- toyDF[,1:3] + 1
#toyDF[,4:6] <- toyDF[,4:6] - 1
#toyDF[toyDF > 5] <- 5
#toyDF[toyDF < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
toyDF <- as.data.frame(
lapply(toyDF, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(toyDF)
install.packages("randomNames")
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
df<-randomNames(5, gender=1)
df
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
df<-randomNames(5, gender=0)
df
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
#df<-randomNames(5, gender=0)
df <- randomNames(5, gender=1, ethnicity=6, which.names="first")
df
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
#df<-randomNames(5, gender=0)
df <- randomNames(5, gender=0, ethnicity=6, which.names="first")
df
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
#df<-randomNames(5, gender=0)
pnames <- randomNames(5, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
users
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
#df<-randomNames(5, gender=0)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
head(users)
library(randomNames)
library(tidyverse)
library(kableExtra)
library(knitr)
#df<-randomNames(5, gender=0)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
users
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .3
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of person randomly (which choose people from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .15
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of person randomly (which choose people from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .2
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of person randomly (which choose people from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .18
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of person randomly (which choose people from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .2
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of persons randomly (which choose male from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .2
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of persons randomly (which choose male from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
# myData <- as.data.frame(
#     lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
#     row.names = users,
#     col.names = items)
# knitr::kable(myData)
dd=dim(myData)
nna=20/100 #overall
df1<-myData
df1[matrix(rbinom(prod(dd), size=1,prob=nna)==1,nrow=dd[1])]<-NA
df1
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .2
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of persons randomly (which choose male from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
## Test introducing item bias (see summary)
#myData[,1:3] <- myData[,1:3] + 1
#myData[,4:6] <- myData[,4:6] - 1
#myData[myData > 5] <- 5
#myData[myData < 1] <- 1
# add NAs via: https://stackoverflow.com/questions/27454265/r-randomly-insert-nas-into-dataframe-proportionaly
# myData <- as.data.frame(
#     lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
#     row.names = users,
#     col.names = items)
# knitr::kable(myData)
dd=dim(myData)
nna=.2 #overall
df1<-myData
df1[matrix(rbinom(prod(dd), size=1,prob=nna)==1,nrow=dd[1])]<-NA
df1
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .2
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of persons randomly (which choose male from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# We can alos use binomial distribution like this:
# dd=dim(myData)
# nna=.2 #overall
# df1<-myData
# df1[matrix(rbinom(prod(dd), size=1,prob=nna)==1,nrow=dd[1])]<-NA
# df1
# randomly create a toy square matrix with n users and items, ranking 1 to 5
# set simulation vars
set.seed(12345)
n <- 10
percentNAs <- .18
probs <- c(1-percentNAs, percentNAs)
#use randomNames package to get names of persons randomly (which choose male from middle eastern ethnicity)
pnames <- randomNames(10, gender=0, ethnicity=6, which.names="first")
users <- c(pnames)
items <- paste("Item_", LETTERS[1:n], sep="")
# generate data
myData <- as.data.frame(replicate(n, floor(runif(n, 1,6))))
myData <- as.data.frame(
lapply(myData, function(df) df[sample(c(TRUE, NA), prob = probs, size = length(df), replace = TRUE) ]),
row.names = users,
col.names = items)
knitr::kable(myData)
# We can alos use binomial distribution like this:
# dd=dim(myData)
# nna=.2 #overall
# df1<-myData
# df1[matrix(rbinom(prod(dd), size=1,prob=nna)==1,nrow=dd[1])]<-NA
# df1
training_portion <- sample(n, size = floor(n * 0.75), replace=F)
training.data <- myData[training_portion, ]
testing.data <- myData[-training_portion, ]
training_portion <- sample(n, size = floor(n * 0.75), replace=F)
training.data <- myData[training_portion, ]
testing.data <- myData[-training_portion, ]
training.data
training_portion <- sample(n, size = floor(n * 0.75), replace=F)
training.data <- myData[training_portion, ]
testing.data <- myData[-training_portion, ]
testing.data
# raw average rating (training set)
raw_avg_train <- mean(as.matrix(training.data), na.rm=TRUE)
raw_avg_train
RMSE <- function(observed, predicted) {
# get RMSE
sqrt(mean((observed - predicted)^2, na.rm=TRUE))
}
# the RMSE for raw average for training set
RMSE_training.data <- RMSE(as.matrix(training.data), raw_avg_train)
RMSE_training.data
# the RMSE for raw average for test set
RMSE_testing.data <- RMSE(as.matrix(testing.data), raw_avg_train)
RMSE_testing.data
user_bias_training <- rowMeans(as.matrix(training.data), na.rm = T) - raw_avg_train
user_bias_testing <- rowMeans(as.matrix(testing.data), na.rm = T) - raw_avg_train
item_bias_training <- colMeans(as.matrix(training.data), na.rm = T) - raw_avg_train
user_bias_training
user_bias_testing
item_bias_training
baseline_predictor <- function(rawAvg, userBias, itemBias) {
# calculates predictions based on rawaverage and user and item bias calcs
userlist <- names(userBias)
itemlist <- names(itemBias)
df <- data.frame()
for (i in userBias) {
UserPred <- rawAvg + i + itemBias
df <- rbind(df, UserPred)
}
# Like the videos lectures suggests, we should not exceed our max and min values
df[df > 5] <- 5
df[df < 1] <- 1
row.names(df) <- userlist
names(df) <- itemlist
df
}
trainPredictions <- baseline_predictor(raw_avg_train, user_bias_training, item_bias_training)
heatmap(as.matrix(trainPredictions), Rowv=NA, Colv=NA, scale = 'none', col = heat.colors(256))
knitr::kable(trainPredictions)
testPredictions <- baseline_predictor(raw_avg_train, user_bias_testing, item_bias_training)
knitr::kable(testPredictions)
baseline_predictor <- function(rawAvg, userBias, itemBias) {
# calculates predictions based on rawaverage and user and item bias calcs
userlist <- names(userBias)
itemlist <- names(itemBias)
df <- data.frame()
for (i in userBias) {
UserPred <- rawAvg + i + itemBias
df <- rbind(df, UserPred)
}
# Like the videos lectures suggests, we should not exceed our max and min values
df[df > 5] <- 5
df[df < 1] <- 1
row.names(df) <- userlist
names(df) <- itemlist
df
}
training_Predictions <- baseline_predictor(raw_avg_train, user_bias_training, item_bias_training)
heatmap(as.matrix(training_Predictions), Rowv=NA, Colv=NA, scale = 'none', col = heat.colors(256))
knitr::kable(training_Predictions)
testing_Predictions <- baseline_predictor(raw_avg_train, user_bias_testing, item_bias_training)
knitr::kable(testing_Predictions)
RMSE_training_Predictions <- RMSE(as.matrix(training.data), as.matrix(training_Predictions))
RMSE_training_Predictions
RMSE_testing_Prediction <- RMSE(as.matrix(testing.data), as.matrix(testing_Predictions))
RMSE_testing_Prediction
RMSE_training_Predictions <- RMSE(as.matrix(training.data), as.matrix(training_Predictions))
RMSE_training_Predictions
RMSE_testing_Predictions <- RMSE(as.matrix(testing.data), as.matrix(testing_Predictions))
RMSE_testing_Predictions
# the RMSE for the baseline predictors for training set
RMSE_training_Predictions <- RMSE(as.matrix(training.data), as.matrix(training_Predictions))
RMSE_training_Predictions
# the RMSE for the baseline predictors for test data
RMSE_testing_Predictions <- RMSE(as.matrix(testing.data), as.matrix(testing_Predictions))
RMSE_testing_Predictions
trainVec <- c(RMSE_training.data,
RMSE_training_Predictions,
(1-RMSE_training_Predictions/RMSE_training.data)*100)
testVec <- c(RMSE_testing.data,
RMSE_testing_Predictions,
(1-RMSE_testing_Predictions/RMSE_testing.data)*100)
summary <- data.frame(trainVec, testVec)
names(summary) <- c("train", "test")
row.names(summary) <- c("Raw Average RMSE",
"Simple Predictor RMSE",
"Percent Improvement")
knitr::kable(summary)
trainVec <- c(RMSE_training.data,
RMSE_training_Predictions,
(1-RMSE_training_Predictions/RMSE_training.data)*100)
testVec <- c(RMSE_testing.data,
RMSE_testing_Predictions,
(1-RMSE_testing_Predictions/RMSE_testing.data)*100)
summary <- data.frame(trainVec, testVec)
names(summary) <- c("Training", "Testing")
row.names(summary) <- c("Raw Average RMSE",
"Simple Predictor RMSE",
"Percent Improvement")
knitr::kable(summary)
setwd("C:/Desktop.6.20.2020/Data Science Semester 3 Summer/Data612 - Recommender Systems/Data612/Project3")
knitr::opts_chunk$set(echo = TRUE)
library(recommenderlab)
library(ggplot2)
library(dplyr)
library(tidyr)
# reading Data
data(MovieLense)
movie_matrix<-MovieLense
# reading Data
data(MovieLense)
movie_matrix<-MovieLense
# creating evaluation scheme
set.seed(123)
# 5-fold CV; everything that is above 3 is considered a good rating; 5 neighbours will be find for a given user(item) to make recommendation
es<- evaluationScheme(movie_matrix, method = "cross", train = 0.9, given = 5, goodRating = 3, k = 5)
#  testing user-based CF using centered data and Pearson coefficient as a similarity measure to find neighbours
param_ubcf<-list(normalize="center", method = "Cosine")
result_1<- evaluate(es, method = "UBCF", type = "ratings", param = param_ubcf)
avg(result_1)
#  testing SVD method using the following parametrs: k = 10, maxiter = 100, normalize = center
param_svd = list(normalize="center", maxiter = 100, k =100)
result_2<- evaluate(es, method = "SVD",  param = param_svd, type = "ratings")
avg(result_2)
# testing funk SVD method using the following paramentrs: k	 =  10, gamma	 =  0.015, lambda	 =  0.001, normalize	 =  center, min_epochs	 =  50, max_epochs	 =  200
param_svdf<- list(normalize="center", k = 10, gamma	 =  0.015,lambda	 =  0.001, min_epochs	 =  50, max_epochs	 =  200)
result_3<- evaluate(es, method = "SVDF", type = "ratings", param = param_svdf)
avg(result_3)
# testing funk SVD method using the following paramentrs: k	 =  10, gamma	 =  0.015, lambda	 =  0.001, normalize	 =  center, min_epochs	 =  50, max_epochs	 =  200
param_svdf<- list(normalize="center", k = 10, gamma	 =  0.015,lambda	 =  0.001, min_epochs	 =  50, max_epochs	 =  200)
result_3<- evaluate(es, method = "SVDF", type = "ratings", param = param_svdf)
avg(result_3)
m1<-cbind(RMSE=avg(result_1))
m2<-cbind(RMSE=avg(result_2))
m3<-cbind(RMSE=avg(result_3))
summary = rbind(m1, m2, m3)
rownames(summary) <- c("UBCF","SVD","FUNK SVD")
summary
algorithms <- list(
"user-based" = list(name="UBCF", param=list(normalize="center", method = "Cosine")),
"SVD" = list(name="SVD", param=list(normalize="center", maxiter = 100, k =100)),
"Funk SVD" = list(name = "SVDF", param = list(normalize="center", k = 10, gamma	 =  0.015,lambda	 =  0.001, min_epochs	 =  50, max_epochs	 =  200))
)
results <- evaluate(es, algorithms, n=c(5, 10, 15, 20))
plot(results, annotate = 1:4, legend="topleft", main = "ROC")
# splitting data on train and test sets
esf<- evaluationScheme(movie_matrix, method = "split", train = 0.9, given = 5, goodRating = 3)
train <-getData(esf, "train")
test <-getData(esf, "unknown")
test_known <- getData(esf, "known")
#  building user-based recommendation model
param_f<- list(normalize = "center", maxiter = 100, k =100)
final_model <- Recommender(train, method = "SVD", param = param_f)
final_model
# getting recommendations (top 10)
final_prediction<- predict (final_model, test, n = 10, type = "topNList")
final_prediction@items[1]
final_prediction@ratings[1]
